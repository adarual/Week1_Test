{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2dfcc82",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c2686a",
   "metadata": {},
   "source": [
    "The goal of this notebook is to build a Linear Regression Model on the California Housing Dataset.\n",
    "\n",
    "We will be trying to predict the MedianPrice per house in each block. Denoted as the target column. \n",
    "\n",
    "This price is in $100,000s\n",
    "\n",
    "Previously we performed some initial data cleaning on this data and we will progress using that.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7256ae73",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/housing_data_cleaned_IQR.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# loading our cleaned data into  data frames\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# We can use these different cleaning methods to experiment with our model and see the impact\u001b[39;00m\n\u001b[0;32m      9\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/housing_data_cleaned.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m df_IQR \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/housing_data_cleaned_IQR.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# tukey cleaned\u001b[39;00m\n\u001b[0;32m     11\u001b[0m df_STD \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/housing_data_cleaned_STD.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/housing_data_cleaned_IQR.csv'"
     ]
    }
   ],
   "source": [
    "# importing pandas package\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# loading our cleaned data into  data frames\n",
    "# We can use these different cleaning methods to experiment with our model and see the impact\n",
    "df = pd.read_csv('data/housing_data_cleaned.csv')\n",
    "df_IQR = pd.read_csv('data/housing_data_cleaned_IQR.csv') # tukey cleaned\n",
    "df_STD = pd.read_csv('data/housing_data_cleaned_STD.csv') # mahalanobis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fe8b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify our data frames looking at their info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbf5af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify our data frames looking at their info\n",
    "df_STD.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86354478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify our data frames looking at their info\n",
    "df_IQR.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dd649e",
   "metadata": {},
   "source": [
    "### Recap of dataset characteristics\n",
    "\n",
    "California Housing dataset\n",
    "Data Set Characteristics:\n",
    "\n",
    "* Number of Instances: 20640\n",
    "\n",
    "* Number of Attributes: 8 numeric, predictive attributes and the target\n",
    "\n",
    "* Attribute Information:\n",
    "    - MedInc        median income in block group\n",
    "    \n",
    "    - HouseAge      median house age in block group\n",
    "    \n",
    "    - AveRooms      average number of rooms per household\n",
    "    \n",
    "    - AveBedrms     average number of bedrooms per household\n",
    "    \n",
    "    - Population    block group population\n",
    "    \n",
    "    - AveOccup      average number of household members\n",
    "    \n",
    "    - Latitude      block group latitude\n",
    "    \n",
    "    - Longitude     block group longitude\n",
    "    \n",
    "This dataset was obtained from the StatLib repository. https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
    "\n",
    "The target variable is the median house value for California districts, expressed in hundreds of thousands of dollars ($100,000).\n",
    "\n",
    "This dataset was derived from the 1990 U.S. census, using one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people).\n",
    "\n",
    "An household is a group of people residing within a home. Since the average number of rooms and bedrooms in this dataset are provided per household, these columns may take surpinsingly large values for block groups with few households and many empty houses, such as vacation resorts.\n",
    "\n",
    "It can be downloaded/loaded using the :func:sklearn.datasets.fetch_california_housing function.\n",
    "\n",
    ".. topic:: References\n",
    "\n",
    "- Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
    "  Statistics and Probability Letters, 33 (1997) 291-297"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8111b7",
   "metadata": {},
   "source": [
    "### Note\n",
    "Moving forward we will include all outliers. Once we have run through building a model once, your task will be to train another using a data set where the outliers have been removed and compare models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfc6ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing the head of the df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7f9794",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# viewing the info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e00e3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing the distributions\n",
    "df.hist(bins = 50, figsize=(20,20));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560f6cdd",
   "metadata": {},
   "source": [
    "# Linear Regression assumptions\n",
    "\n",
    "Linear regression assumes that each target variables are linearly correlated with the target variable\n",
    "- We need to investiage this\n",
    "\n",
    "No multi-colinearity, i.e none of the features are highly correlated with each other, makes regression bad\n",
    "- We need to investigate this also\n",
    "\n",
    "Independence\n",
    "- We will assume this to be the case\n",
    " - (can use the Durbin-Watson test, to asses, but we will not be covering this)\n",
    "\n",
    "There are some other assumptions that we can test for after fitting the model \n",
    "- Homoscedasticity, by using Residual vs Predicted value plots\n",
    "- Multivariate Normality, using Q-Q plots\n",
    " - (There are more formal tests for Multivariate Normality but we will not be covering those, namely <br>\n",
    "   Shapiro-Wilk, Kolmogorov-Smironov, Jarque-Barre, or Dâ€™Agostino-Pearson)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0ddf35",
   "metadata": {},
   "source": [
    "#### Assesing if features are linearly correlated with the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3487c94e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plotting scatter of all columns against eachother\n",
    "pd.plotting.scatter_matrix(df,figsize = (20,20));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51511160",
   "metadata": {},
   "source": [
    "#### Discuss what you can observe from above, solely related to correlation with the target variable?\n",
    "\n",
    "#### Discuss what you observe related to correlation of the features themselves?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69893d40",
   "metadata": {},
   "source": [
    "## Lets retrieve the correlation coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5a5d02",
   "metadata": {},
   "source": [
    "#### We can use the basic Pandas .corr() plot to see the <em>r</em>  values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d2bb01",
   "metadata": {},
   "source": [
    "The <b><i>r</b></i> value or Pearson Correlation Coefficient is a measure of the linear interdependence between two variables, or two sets of data.\n",
    "\n",
    "The Pearson correlation coefficient is probably the most widely used measure for linear relationships between two normal distributed variables and thus often just called \"correlation coefficient\". <br>\n",
    "However there other measures that are more sophisticaed such as Spearman's.<br>\n",
    "\n",
    "Here is a straight forward explanation of how Pearson and Spearman correlation differ:<br>\n",
    "https://support.minitab.com/en-us/minitab/21/help-and-how-to/statistics/basic-statistics/supporting-topics/correlation-and-covariance/a-comparison-of-the-pearson-and-spearman-correlation-methods/\n",
    "\n",
    "Pandas has Pearsons, Spearmans and the Kendall-Tau correlations all built in.<br>\n",
    "Easily accessable using the method argument\n",
    "\n",
    "Usually, the Pearson coefficient is obtained via a Least-Squares fit and a value of 1 represents a perfect positive relation-ship, -1 a perfect negative relationship, and 0 indicates the absence of a relationship between variables.\n",
    "\n",
    "Calculation is below for completeness:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b58392c",
   "metadata": {},
   "source": [
    "$$r = \\frac{{}\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})}\n",
    "{\\sqrt{\\sum_{i=1}^{n} (x_i - \\overline{x})^2(y_i - \\overline{y})^2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b018c865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note you do not need to worry about recaling the data calling pandas.corr()\n",
    "# Pearsons Correlation is a normalised measure\n",
    "df.corr(method = 'pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb326fe-0b69-4a78-b12f-94aa4337c981",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr(method = 'spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf367c7",
   "metadata": {},
   "source": [
    "#### We can feed this into Seaborn to add more colour to our results with a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90db5973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating an axes on which to draw the heatmap\n",
    "fig, ax =  plt.subplots(figsize = (10,10))\n",
    "\n",
    "# Note: there is an exercise sheet on Matplot lib which you may find useful in understanding about figures and axes\n",
    "\n",
    "# Plotting the heatmap on the axes\n",
    "sns.heatmap(df.corr(method='pearson'), annot=True, linewidths=0.8, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e66d99",
   "metadata": {},
   "source": [
    "- Median income has the highest correlation value 0.688 which indicates that it is linked to the target variable.  \n",
    "\n",
    "- Longitude and latitude have negative correlation as it is inversely proportional to the target variable.\n",
    "\n",
    "- Can you explain what this means in real terms?\n",
    "  \n",
    "- Comment on all other <b><i>r</b></i> values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa473e7",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary style=\"color:green;font-weight:bold\">Thoughts</summary>\n",
    "    \n",
    "Blocks with higher median incomes have higher median house prices, to be expected\n",
    "    \n",
    "Logtitude and lattitdue being negatively correlated suggest that further south and east have higher house prices, we will visualize this later\n",
    "    \n",
    "Also they are heavily correlated together\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87078ad2",
   "metadata": {},
   "source": [
    "#### Right now we are just interested in the correlation of each feature with the target variable so we can isolate this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca52a85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calling the correlation method, but this time on just one column, rather than the whole DataFrame\n",
    "df.corr(method='pearson')['Target'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6ccb0f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## The next step is to Re-Scale the data\n",
    "\n",
    "We will scale the features before building an OLS model on top of it. This is because SciKit-Learn will expect an intercept of 0 to build the model. By standardising the data we ensure that the line of best fit will pass through the origin as the mean of all observations will be zero.\n",
    "\n",
    "\n",
    "- But before that we need to split the data\n",
    "\n",
    "- If we rescale over the entire data, i.e before we split the data, we will induce what is referred to as <b><i>Data Leakage</b></i>.\n",
    "\n",
    "This refers to information from our test set (which represents the **unseen**) having an impact on our model building process, thus not truly being unseen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5720c86",
   "metadata": {},
   "source": [
    "## Importance of the Train Test split and how it is done\n",
    "\n",
    "It is crucial that we test our model on data that it has not seen before so we can gauge how well it might perform in the real world on real unseen data.\n",
    "\n",
    "An important part of splitting the data is to ensure there is a broad range of of the target variable in each split.\n",
    "\n",
    "If we didn't ensure this, it is possible that we could train our model on the lowest target values and then test it on the highest. This would severly bias our model and the underperformance would be a Reducible Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f220cefe",
   "metadata": {},
   "source": [
    "Scikit-learn's `train_test_split` ensures that we have a similar distribution of our target variable across both the training and testing sets. \n",
    "\n",
    "We will split the data and observe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb6b377-4be4-422b-96dd-ff34509e60c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d411f9-2f58-40ca-aef8-bccf0a6f3886",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12734943-743e-44db-95e4-151c7391b1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['MedInc', \n",
    "    'HouseAge', \n",
    "    'AveRooms', \n",
    "    'AveBedrms', \n",
    "    'Population', \n",
    "    'AveOccup',\n",
    "    'Latitude', \n",
    "    'Longitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5f3335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing package\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[['MedInc', \n",
    "        'HouseAge', \n",
    "        'AveRooms', \n",
    "        'AveBedrms', \n",
    "        'Population', \n",
    "        'AveOccup',\n",
    "        'Latitude', \n",
    "        'Longitude']]\n",
    "\n",
    "y = df['Target']\n",
    "\n",
    "#Splitting the data into test and train sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state =42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f999b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualising the distribution of the target in the training set\n",
    "y_train.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a918151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualising the distribution of the target in the training set\n",
    "y_test.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966e7ecb",
   "metadata": {},
   "source": [
    "We have near identical distributions across both sets.\n",
    "\n",
    "This ensures that both the training and testing contain a representitive range of the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d51858",
   "metadata": {},
   "source": [
    "## Next \n",
    "We fit and apply scaling to our Training data, then apply this scaling algorithm to the Test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3b216e",
   "metadata": {},
   "source": [
    "We will be applying StandardScalar() to the data, this is transforming the data to thier Z scores, ensuring they have a mean of 0 and a Standard Deviation of 1.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ed4e70",
   "metadata": {},
   "source": [
    "$$z= \\frac{x - \\mu}{\\sigma}$$\n",
    "<br>\n",
    "\n",
    "<center>$z$ is the rescaled value </center>\n",
    "<center>$x$ is the original value </center>\n",
    "<center>$\\mu$ is the mean </center>\n",
    "<center>$\\sigma$ is the standard deviation</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e857f8",
   "metadata": {},
   "source": [
    "In this case we are manually scaling the target variable\n",
    "\n",
    "The downside of this is we must reverse this transform later in order to get the error metrics\n",
    "\n",
    "There are more sophisticaed methods to do this in Sci-Kit learn using Pipelines and Transformed Target Regressor\n",
    "\n",
    "But we will progress by hand to understand what is happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6d80be-330c-4d1f-9af1-a25f7305890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Thom\" #variable\n",
    "myfunc = print #function\n",
    "StandardScaler # class -> object (i.e. a piece of data). StandardScaler() == create a standard scaler object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c804be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the package\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Calling the method twice, once for the features and once for the target\n",
    "scaler =  StandardScaler()\n",
    "scaler_target = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70742dd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sci-Kit learn returns an array by default, we will be putting the data into another dataframe, \n",
    "# Extracting the column names for this purpose\n",
    "cols = list(X_train.columns)\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f8adee-b54f-40bf-856e-f78c8d579711",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bcd91f-aaae-42d8-9755-4beb97eed810",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scaler.fit_transform(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4c0cbe-22bf-4221-b704-074dee5f492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the scaler on the training features and applying it to both the training and test data\n",
    "# Note how the Scaler is only fit to the training set, but then applied to both\n",
    "X_train[cols] = scaler.fit_transform(X_train)\n",
    "X_test[cols] = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01996e89-e5dc-4362-8feb-9b90b3dce893",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbf8159",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Fitting the scaler on the training target and applying it to both the training and test data\n",
    "y_train = scaler_target.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test = scaler_target.transform(y_test.values.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77128a39-adac-449e-a125-8081a7b70e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train#.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf93f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the transformed training data\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d7cb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the transformed test data\n",
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74447947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observing for the target \n",
    "print(y_train.std(), y_train.mean())\n",
    "print(y_test.std(), y_test.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db078933",
   "metadata": {},
   "source": [
    "## Lets build our first model\n",
    "\n",
    "We will begin with a simple linear model with one input variable or feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dca5c6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Viewing columns that can be selected as features for our model\n",
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0abeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the features that we will use\n",
    "cols = ['MedInc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5407180",
   "metadata": {},
   "source": [
    "More info on the model and parameters available<br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96639f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary package\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# call the method\n",
    "regressor = LinearRegression(fit_intercept = False)\n",
    "\n",
    "# Fit the model <- THIS IS ML\n",
    "regressor.fit(X_train[cols], y_train)\n",
    "\n",
    "# Carry out predictions on our test set\n",
    "y_pred = regressor.predict(X_test[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56a9bde-be1d-4e60-a8e4-0970a9dfbc28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4863e1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the predictions\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32abbeb",
   "metadata": {},
   "source": [
    "# What just happened?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a275b6",
   "metadata": {},
   "source": [
    "Scikit-Learn built a simple linear regression on the standardised training data using OLS, we then used the test data to make predictions, hence testing the model on data it had not seen before.\n",
    "\n",
    "Below we will display the training data and the model created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f44392",
   "metadata": {},
   "source": [
    "<center>Simple linear regression</center>\n",
    "\n",
    "$$ y = mx+c $$ \n",
    "    \n",
    "<br>\n",
    "<center>$y =$ target - MedPrice</center>\n",
    "<center>$x =$ MedInc</center>\n",
    "<center>$m =$ Coefficient (gradient) from the model</center>\n",
    "<center>$c =$Intercept</center> \n",
    "<center>which will be (0,0) as we standardized all our scores, i.e both X and Y have a mean of zero</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85931f61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# accessing model coefficient, the gradient\n",
    "regressor.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728f41f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing intercept\n",
    "regressor.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19c0fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing feature used\n",
    "regressor.feature_names_in_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a804b991",
   "metadata": {},
   "source": [
    "# Plotting Model against training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dc34f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "# Plotting MedInc against the Target\n",
    "plt.scatter(X_train[cols], y_train, s=0.1)\n",
    "\n",
    "\n",
    "# Selecting x values to input into the model\n",
    "# Started at the min value of MedInc that we have, \n",
    "# the upper limit was found by experimenitng so as not to exceed the y value\n",
    "# in this case the linear model is very poor at predicting anything over a standardised x value of over 3\n",
    "x = np.linspace(X_train[cols].min(), 3.6, 100)\n",
    "\n",
    "# Using the gradient and intercept from the model to plot \n",
    "# The intercept is zero but it is added for completeness\n",
    "# y = m              *x+ c\n",
    "y = (regressor.coef_)*x+(regressor.intercept_)\n",
    "\n",
    "# # Plotting the linear model\n",
    "plt.plot(x, y, '-r')\n",
    "\n",
    "# # Adding labels to each axis\n",
    "plt.xlabel('MedInc Z_scores')\n",
    "plt.ylabel('Target Z_scores')\n",
    "\n",
    "# calling the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eb5f02",
   "metadata": {},
   "source": [
    "# Plotting model against Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c0ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above\n",
    "# notice the difference in the scatter plot\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.scatter(X_test[cols], y_test, s=0.1)\n",
    "x = np.linspace(X_train[cols].min(),3.6,100)\n",
    "y = (regressor.coef_)*x+(regressor.intercept_)\n",
    "\n",
    "plt.plot(x, y, '-r', label='Model')\n",
    "plt.xlabel('MedInc Z_scores')\n",
    "plt.ylabel('Target Z_scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3befa722",
   "metadata": {},
   "source": [
    "#### Assesing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11fbad6",
   "metadata": {},
   "source": [
    "As we have scaled the target variable, it makes sense to descale it to measure the RMSE, Root Mean Square Error of the model\n",
    "\n",
    "If we leave it scaled the units won't make sense, however after we convert it back and find the RMSE then the error is in the same units as the data was in the first place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e62e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to reverse the transformation so we have meanginful results\n",
    "# calling inverse_transform on our scaler\n",
    "y_true = scaler_target.inverse_transform(y_test) # <- observed y values\n",
    "y_pred_inverse = scaler_target.inverse_transform(y_pred) # <- predicted y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4effa264",
   "metadata": {},
   "source": [
    "Now to measure the errors\n",
    "\n",
    "These errors are the differences from our predicted target values to the actual target values of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3b85e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Calculating the errors\n",
    "mae = mean_absolute_error(y_true, y_pred_inverse)\n",
    "mse = mean_squared_error(y_true, y_pred_inverse)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Calculting the R^2\n",
    "r2 = r2_score(y_true, y_pred_inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9864c149-5103-4144-b471-d3c97944bb88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee8695e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Viewing these results\n",
    "print('MAE = ', mae)\n",
    "print('MSE = ', mse)\n",
    "print('RMSE = ', rmse)\n",
    "print('R^2 = ', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a6989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can view the coefficients of the features\n",
    "regressor.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d139f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The feature names included\n",
    "regressor.feature_names_in_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215c9750",
   "metadata": {},
   "source": [
    "These results are in line with what we found before using the correlation matrix<br>\n",
    "The $r^2$ value is roughly the square of the correlation coefficitent of 0.688 that we originally got.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e25ec3",
   "metadata": {},
   "source": [
    "# Multi-Linear Regression\n",
    "\n",
    "We will now add in another variable and see its impact on the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd5d1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8600ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols =['MedInc','AveRooms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed1fa02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# call the method\n",
    "regressor = LinearRegression( fit_intercept = False)\n",
    "\n",
    "# Fit the model\n",
    "regressor.fit(X_train[cols],y_train)\n",
    "\n",
    "# Carry out predictions on our test set\n",
    "y_pred = regressor.predict(X_test[cols])\n",
    "\n",
    "# first we need to reverse the transformation so we have meanginful results\n",
    "y_true = scaler_target.inverse_transform(y_test)\n",
    "y_pred = scaler_target.inverse_transform(y_pred)\n",
    "\n",
    "# Calculating the errors\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Calculting the R^2\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# Viewing these results\n",
    "print('MAE = ', mae.round(4))\n",
    "print('MSE = ', mse.round(4))\n",
    "print('RMSE = ', rmse.round(4))\n",
    "print('R^2 = ', r2.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dff686",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.score(X_test[cols],y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4990cd",
   "metadata": {},
   "source": [
    "By adding in AveRooms, we made minimal difference to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5678608",
   "metadata": {},
   "source": [
    "What do the above figures represent?<br>\n",
    "\n",
    "\n",
    "MAE =  0.6236  <br>\n",
    "A mean absolute error of 0.624 means that on average we are $60,000 off predicting the median price of a house in a block.\n",
    "\n",
    "MSE =  0.7096<br>\n",
    "The mean square error is what the model used to build itself, it has no physical meaning, techincally it is in (dollars/100,000)^2\n",
    "\n",
    "RMSE =  0.8424<br>\n",
    "A root mean square error of 0.84 tell us that we are $84,000 on average off when predicting.<br> This measure as opposed to MAE is affected more by predictions that were further off.<br> Our <i>worst</i> case scenarios have inflated this as opposed to the MAE.\n",
    "\n",
    "R^2 =  0.4778<br>\n",
    "The coefficient of determination  is defined as\n",
    "<centre><b><em>1-(u/v)</em></b></centre>, where<br> <b><em>u</em></b> is the residual sum of squares ((y_true - y_pred)^ 2).sum() and<br> <b><em>v</em></b> is the total sum of squares ((y_true - y_true.mean()) ^ 2).sum(). <br>The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3b3eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abc14063",
   "metadata": {},
   "source": [
    "## Trying different combinations of features\n",
    "\n",
    "By using some built in methods in Python we can experiment with different combinations of features and asses the quality of the model.<br>\n",
    "Below I will try every possible combination of 2 features and record the RMSE for each.\n",
    "\n",
    "When finished, we will look at the top and bottom 5 combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eccdc71",
   "metadata": {},
   "source": [
    "The method used will be <em>combinations</em> from the built in <em>itertools</em> package <br> You can find out more here<br>https://www.geeksforgeeks.org/itertools-combinations-module-python-print-possible-combinations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad601bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick demo\n",
    "# importing required packages\n",
    "from itertools import combinations\n",
    "\n",
    "# Combinations takes two arguments\n",
    "# The first is  some kind if iterable\n",
    "# The second, how many items we can have in our combinations\n",
    "# print(list(combinations('word',2)))\n",
    "\n",
    "\n",
    "# experiment with the code below\n",
    "possibilities = list(combinations(['A','B','C','D'],2))\n",
    "possibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cf0467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required packages\n",
    "from itertools import combinations\n",
    "\n",
    "# Creating a list of all columns\n",
    "all_features = list(X_train.columns)\n",
    "\n",
    "# List of all possible combinations of 2 columns\n",
    "sub_set = list(combinations(all_features,2))\n",
    "\n",
    "# creating an empty dict to store results in\n",
    "results = {}\n",
    "\n",
    "# looping over each item in the seb set, which is a tuple containing a pair of column names\n",
    "for features in sub_set:\n",
    "    \n",
    "    # creating a list from this tuple which will make it easier to select the columns from pandas\n",
    "    # when selecting columns in pandas you need to supply it with a list\n",
    "    cols = list(features)\n",
    "\n",
    "    # call the method\n",
    "    regressor = LinearRegression(fit_intercept = False)\n",
    "\n",
    "    # Fit the model\n",
    "    regressor.fit(X_train[cols],y_train)\n",
    "\n",
    "    # Carry out predictions on our test set\n",
    "    y_pred = regressor.predict(X_test[cols])\n",
    "\n",
    "    # first we need to reverse the transformation so we have meanginful results\n",
    "    y_true = scaler_target.inverse_transform(y_test)\n",
    "    y_pred = scaler_target.inverse_transform(y_pred)\n",
    "\n",
    "    # Calculating the errors\n",
    "    # Only RMSE is needed right now, but the rest are here for completeness\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    # Calculting the R^2\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # Creating an entry in the results dictionary\n",
    "    # The ket is the column names\n",
    "    # The value is the RMSE\n",
    "    results[str(cols)] = rmse\n",
    "\n",
    "    \n",
    "# After we have looped through all combinations and saved thee values to the dict\n",
    "# A dataframe is created from the dict\n",
    "df_results = pd.DataFrame.from_dict(results, orient = 'index')\n",
    "\n",
    "# The dataframe uses the keys as the index, i dont want this so i reset the index\n",
    "df_results.reset_index(inplace=True)\n",
    "\n",
    "# Rename the columns to reflect their content\n",
    "df_results.columns = ['Features','RMSE']\n",
    "\n",
    "# Sort by RMSE\n",
    "df_results.sort_values(by='RMSE', inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cecf25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf1e5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0bb81c",
   "metadata": {},
   "source": [
    "#### It seems reasonable that if we chose a model with 2 features, we should select MedInc and HouseAge\n",
    "- The have correlation with the target variable\n",
    "- Adding HouseAge, that has low correltion with the target variable on its own, does seem to help the model\n",
    "- There is no multicolinearity between them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776b8f21",
   "metadata": {},
   "source": [
    "## Task\n",
    "### Review the notebook\n",
    "### Fit a model using all features\n",
    "### Then \\\\/\\\\/\\\\/\\\\/\n",
    "### Iterate through other combinations of features and record their RMSE in a Data Frame as above\n",
    "\n",
    "#### <i>Extension, see if you can loop through all possible combinations</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8de38e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9033dbbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe7c748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1868d8e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ac2e82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca365a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124a0f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516f622d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77693c12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3df682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004931b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22d01124",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary style=\"color:green;font-weight:bold\">Solution</summary>\n",
    "\n",
    "    # defining list of al columns\n",
    "    all_features = list(X_train.columns)\n",
    "    \n",
    "    # creating a dictionary to hold results\n",
    "    results = {}\n",
    "\n",
    "    # Looping through number of possible combinations of features\n",
    "    for number_of_features in range(1,len(X_train.columns)+1):\n",
    "\n",
    "        # create all combinations of this number\n",
    "        sub_set = list(combinations(all_features,number_of_features))\n",
    "\n",
    "        # looping through all combinations\n",
    "        for features in sub_set:\n",
    "    \n",
    "            # defining the list of features from this combination\n",
    "            cols = list(features)\n",
    "\n",
    "            # call the method\n",
    "            regressor = LinearRegression( fit_intercept = False)\n",
    "\n",
    "            # Fit the model\n",
    "            regressor.fit(X_train[cols],y_train)\n",
    "\n",
    "            # Carry out predictions on our test set\n",
    "            y_pred = regressor.predict(X_test[cols])\n",
    "\n",
    "            # first we need to reverse the transformation so we have meanginful results\n",
    "            y_true = scaler_target.inverse_transform(y_test)\n",
    "            y_pred = scaler_target.inverse_transform(y_pred)\n",
    "\n",
    "            # Calculating the errors\n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            mse = mean_squared_error(y_true, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "\n",
    "            # Calculting the R^2\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "            results[str(cols)] = rmse\n",
    "\n",
    "    df_results = pd.DataFrame.from_dict(results, orient = 'index')\n",
    "    df_results.reset_index(inplace=True)\n",
    "    df_results.columns = ['Features','RMSE']\n",
    "    df_results.sort_values(by='RMSE', inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48fa0c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db26c12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788f8a57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc3df180",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "The lowest RMSE we get is with \n",
    "'MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'AveOccup', 'Latitude', 'Longitude'<br>\n",
    "adding 'population' to this lowers the error\n",
    "\n",
    "\n",
    "We may get further improvements if we combined Longtitude and Lattitude somehow as they are heavily correlated, and eliminating multi-colinearity should help our models performance.\n",
    "\n",
    "This will be left for you to think about and take away with you.\n",
    "\n",
    "\n",
    "Below I have plotted Longtitude against Lattitude, colured by the target.\n",
    "\n",
    "It is clear that properties closer to the sea are more expensive.\n",
    "\n",
    "\n",
    "Could we combine these coordinates in a way to classify if blocks were near the sea or not?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d642766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Longitude against Latitude\n",
    "# coloured by the target variable and setting the colour map\n",
    "df.plot.scatter(x='Longitude' , \n",
    "                y = 'Latitude', \n",
    "                c='Target',\n",
    "                s = df['Population']/100,\n",
    "                cmap = 'coolwarm', \n",
    "                figsize = (20,20))\n",
    "\n",
    "\n",
    "# I experimented with some lines to see if I could draw a boundary that may help with classifying the locations\n",
    "# This is quite a crude method, but it would be a starting point to change the measuremnts of Long and Lat\n",
    "# To close to or far from the sea\n",
    "# A better way would be to verify with some geographical data\n",
    "point1 = [df['Longitude'].min(), df['Latitude'].max()-1.5]\n",
    "point2 = [df['Longitude'].max()-2, df['Latitude'].min()]\n",
    "x_values = [point1[0], point2[0]]\n",
    "y_values = [point1[1], point2[1]]\n",
    "\n",
    "\n",
    "plt.plot(x_values, y_values, 'r', linestyle=\"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109007b8",
   "metadata": {},
   "source": [
    "###  Extension Task\n",
    "\n",
    "You can try rebuilding the model with the data sets with outliers removed too see the difference in the models performances\n",
    "\n",
    "Or you could decide on which are outliers yourself and build the model on that data.\n",
    "\n",
    "Write a brief summary of what impact the removal of outliers has had on the model. Would the model be better or worse at predicting the median price of certain types of houses?\n",
    "\n",
    "Has it's ability to predict one kind of house increased? Has another decreased?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2754acdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_STD.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d066fed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IQR.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ae3279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a977541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e9940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SGDRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d042be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fbe5c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8c894e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8867de8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea6b1a39-96b6-4ea7-8f60-be5d0cd46e0b",
   "metadata": {},
   "source": [
    "## Gradient Descent Regression from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba9fbc8-c0c9-428b-a586-4af2786a57e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = np.sin(4 * x) + x + rng.uniform(size=len(x))\n",
    "\n",
    "def residuals(y, yhat):\n",
    "    return y - yhat\n",
    "\n",
    "def mean_square_error(y, yhat):\n",
    "    assert y.shape == yhat.shape\n",
    "    return np.sum(residuals(y, yhat) ** 2) / y.size\n",
    "\n",
    "def d_w0(y, yhat):\n",
    "    return (-2 * np.sum(residuals(y, yhat))) / y.size\n",
    "\n",
    "def d_w1(y, yhat, x):\n",
    "    return (-2 * np.sum(x * residuals(y, yhat))) / y.size\n",
    "\n",
    "#Hyperparameter -> number of iterations/loops/models attempted before final generated\n",
    "epochs = 1000\n",
    "\n",
    "#\n",
    "w1 = 0\n",
    "w0 = 0\n",
    "\n",
    "# Keep track of mse to see if improving or not\n",
    "error = []\n",
    "\n",
    "#Hyperparemeter: Learning rate -> term which scales the change in theta_0/w_0, theta_1, w_1\n",
    "alpha = 0.01\n",
    "\n",
    "for _ in range(epochs):\n",
    "    yhat = w1 * x + w0\n",
    "    #print(f'w0: {w0}, d_w0: {d_w0(y, yhat)}, jump: {alpha * d_w0(y, yhat)}')\n",
    "    w0 -= alpha * d_w0(y, yhat)\n",
    "    w1 -= alpha * d_w1(y, yhat, x)\n",
    "    \n",
    "    error.append(mean_square_error(y, yhat))\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.scatterplot(x=x, y=y, label= 'data')\n",
    "sns.lineplot(x=x, y=yhat, color ='red', label = 'model')\n",
    "    \n",
    "#plt.plot([np.min(x), np.max(x)], [np.min(w1*x + w0), np.max(w1*x + w0)])\n",
    "#plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda3788f-7bd0-4765-a692-4113a8c5ce9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(range(epochs), error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7713ed34-aa0d-485f-afa2-13e722c2f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_square_error(y, yhat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
